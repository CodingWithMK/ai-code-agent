# ğŸ§  AI Code Agent

### A Local, Generative AI (RAG) Code Agent

> An intelligent **code-reading and reasoning agent** built with **LlamaIndex**, **Ollama**, and **LlamaParse**.
> The agent runs **entirely offline**, analyzes local files, retrieves documentation, and generates new code â€” all without sending data to external APIs.

---

## ğŸš€ Overview

**AI Code Agent** is a local RAG-based (Retrieval-Augmented Generation) system that combines:

* **Ollama LLMs** (`mistral`, `codellama`, etc.) for reasoning and generation
* **LlamaIndex** for document parsing, embeddings, and tool orchestration
* **LlamaParse** for extracting structured text from PDF documentation
* **Custom tools** such as `code_reader` to inspect local files

It was built as part of the **Global AI Hub Generative AI Bootcamp** project, focusing on modular agent design and privacy-centric LLM workflows.

---

## ğŸ§© Core Features

| Feature                       | Description                                                                                               |
| ----------------------------- | --------------------------------------------------------------------------------------------------------- |
| ğŸ’¬ **ReAct Agent**            | A reasoning-and-action architecture that dynamically decides when to read, think, or code.                |
| ğŸ§  **Local Ollama Models**    | Uses local LLMs (e.g., `mistral:7b-instruct`, `codellama:7b-instruct`) with no external API calls.        |
| ğŸ§¾ **PDF Knowledge Parsing**  | Automatically parses API documentation PDFs using **LlamaParse** and indexes them via **LlamaIndex**.     |
| ğŸ—‚ï¸ **Vector Search (RAG)**   | Converts parsed text into vector embeddings (`BAAI/bge-m3`) for fast contextual retrieval.                |
| ğŸ§° **Code Reader Tool**       | Reads any file inside the `/data` folder and returns its exact content for code understanding or editing. |
| âš™ï¸ **Flexible Configuration** | Environment variables control all models and services via `.env` / `.env.example`.                        |
| ğŸ” **Local & Private**        | Entirely offline â€” no data leaves your machine.                                                           |
| ğŸ§© **Extendable**             | Easily add new tools (e.g., code writer, file saver, web fetcher).                                        |

---

## ğŸ—ï¸ Tech Stack

* **Language:** Python 3.10+
* **LLM Runtime:** [Ollama](https://ollama.ai)
* **Framework:** [LlamaIndex](https://docs.llamaindex.ai)
* **Parser:** [LlamaParse](https://www.llamaindex.ai/llamaparse)
* **Environment:** [uv package manager](https://docs.astral.sh/uv/)
* **Embeddings:** `BAAI/bge-m3` (local Hugging Face model)
* **Agent Architecture:** ReAct (Reason + Act loop)

---

## ğŸ“‚ Project Structure

```
ai-code-agent/
â”‚
â”œâ”€â”€ _main.py              # Main entrypoint (agent orchestration)
â”œâ”€â”€ code_reader.py        # Custom FunctionTool for reading local code files
â”œâ”€â”€ prompts.py            # Agent purpose and behavior context
â”œâ”€â”€ config.py             # Centralized environment configuration
â”œâ”€â”€ requirements.txt      # Dependency list
â”œâ”€â”€ .env.example          # Safe environment template
â”œâ”€â”€ .gitignore            # Protects secrets (.env, cache, etc.)
â””â”€â”€ /data/                # Local PDFs, source files, and docs to analyze
```

---

## âš™ï¸ Setup & Installation

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/yourusername/ai-code-agent.git
cd ai-code-agent
```

### 2ï¸âƒ£ Create a virtual environment

Using **uv** (recommended):

```bash
uv sync
```

or traditional pip:

```bash
python -m venv venv
source venv/bin/activate     # (Windows: venv\Scripts\activate)
pip install -r requirements.txt
```

### 3ï¸âƒ£ Configure environment variables

Create your local `.env` from the template:

```bash
cp .env.example .env
```

Then open `.env` and fill in your local or secret keys:

```env
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_TIMEOUT=360
OLLAMA_KEEP_ALIVE=-1
OLLAMA_CHAT_MODEL=mistral:7b-instruct
OLLAMA_CODE_MODEL=codellama:7b-instruct
DATA_DIR=./data
LLAMA_CLOUD_API_KEY=your_llamaparse_api_key_here
```

### 4ï¸âƒ£ Pull Ollama models

```bash
ollama pull mistral:7b-instruct
ollama pull codellama:7b-instruct
```

### 5ï¸âƒ£ Run the Ollama service

```bash
ollama serve
```

### 6ï¸âƒ£ Launch the agent

```bash
uv run python _main.py
```

---

## ğŸ§ª Example Interaction

```
Enter a prompt (Press q to quit):
> Read the content of test.py and give me the exact same code back.
```

Agent internally:

1. Detects intent â†’ uses the **code_reader** tool.
2. Reads `/data/test.py`.
3. Returns the exact code inside a Python code block.

Output:

```python
from flask import Flask, jsonify, request

app = Flask(__name__)
# ...
if __name__ == "__main__":
    app.run(debug=True)
```

---

## ğŸ§° Custom Tools

### ğŸ§© `code_reader.py`

```python
from llama_index.core.tools import FunctionTool
import os

def code_reader_func(file_name):
    path = os.path.join("data", file_name)
    try:
        with open(path, "r", encoding="utf-8") as f:
            content = f.read()
            return {"code": content}
    except Exception as e:
        return {"error": str(e)}

code_reader = FunctionTool.from_defaults(
    fn=code_reader_func,
    name="code_reader",
    description="Reads and returns the exact content of a code file from the data directory."
)
```

> This tool allows the agent to â€œopenâ€ and analyze local code files dynamically.

---

## ğŸ§  Agent Architecture

The agent uses **LlamaIndexâ€™s ReActAgent**, which follows a loop:

```
Thought â†’ Action (tool) â†’ Observation â†’ Reasoning â†’ Final Answer
```

* When a user asks about documentation â†’ uses `api_documentation` (RAG engine).
* When a user asks to â€œread codeâ€ â†’ invokes `code_reader`.
* When a user asks to â€œgenerate codeâ€ â†’ directly uses the LLM (`codellama`).

Its reasoning steps are limited (`max_iterations=5`) to prevent â€œoverthinkingâ€ and ensure fast responses.

---

## ğŸ”’ Security & Privacy

* No external API calls from the LLM (Ollama runs locally).
* `.env` file is excluded from version control via `.gitignore`.
* `.env.example` serves as a public, non-secret template.
* You can safely share this repo without exposing API keys.

---

## ğŸ§­ Environment Config Management

* All environment variables are loaded via `config.py`.
* Secure defaults ensure the app runs even without `.env`.
* Secrets such as `LLAMA_CLOUD_API_KEY` are never hardcoded.
* In CI/CD (e.g., GitHub Actions), store secrets under
  **Settings â†’ Secrets â†’ Actions**.

---

## ğŸ§  Future Enhancements

| Planned Feature                        | Description                                       |
| -------------------------------------- | ------------------------------------------------- |
| âœï¸ **Code Writer Tool**                | Generate and save new Python files from prompts.  |
| ğŸ§© **File Saver / Editor**             | Automatically apply LLM-suggested modifications.  |
| ğŸ” **Web Fetcher Tool**                | Retrieve and summarize online documentation.      |
| ğŸ§± **Memory Module**                   | Persistent long-term memory for multi-turn tasks. |
| ğŸª¶ **Async Warm-Up with Progress Bar** | Show model-loading progress during startup.       |

---

## ğŸ¤ Contributing

Contributions are welcome!
Please open a pull request or issue if you:

* Found a bug ğŸ
* Have an idea for a new tool âš’ï¸
* Want to add support for another Ollama model ğŸ’¡

Steps:

```bash
git checkout -b feature/my-new-tool
git commit -m "Add new feature"
git push origin feature/my-new-tool
```

---

## ğŸ“œ License

This project is released under the **MIT License**.
Feel free to modify, extend, and use it for your personal or educational projects.

---

## ğŸ§© Credits

Developed by **Musab Kaya** as part of the
**Global AI Hub â€“ Generative AI Bootcamp 2025**.

---